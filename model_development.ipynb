{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "e04927ed-1141-4bd6-a6af-c382ad3ed41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "#import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Reshape\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaTokenizer, LlamaForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import tensorflow_recommenders as tfrs\n",
    "from transformers import BertTokenizer\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "d4a74e31-15c0-4ffc-89a4-f1c45e39e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "87b9d72c-e5c8-41b9-b181-4a3dc4261240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'lm_head.weight', 'norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.LlamaForCausalLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "61131a2a-d3be-4fbc-83b2-c2019c11ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_tweets_biden.csv')\n",
    "df['binarized'] = np.zeros(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "a975c4f0-3b93-4d44-9e99-4936bf060b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = df[[\"cleaned_tweets\", \"binarized\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "550f85f8-f797-4f60-940a-5b9a048e6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list_full = question_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "5d7f1197-7158-4dbc-b4cb-30f656779a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = question_list_full[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "ebc6e4ca-891b-4088-b51f-88514632c5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8731df58934b858bb902383d3f9c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = []\n",
    "for question in tqdm(question_list['cleaned_tweets'][:1000]):\n",
    "    prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    prompt += f\"### Instruction:{question}\\n\\n### Response:\"\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "    embeddings.append(outputs.hidden_states[-1][:, -1, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "cf66e15e-059d-4a21-8052-b530dba34eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "pickle.dump(embeddings, open(model_name+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "5e816eb4-81ae-4dbc-924c-70cc34ecf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_fixed = np.array(embeddings).reshape((1000, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "088124cf-bfca-4730-a6c1-135af5c240c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "60078441-19e7-4e1a-ba00-04eb6073cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embedding_col\"] = list(embeddings_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "d75c6c04-0958-4793-91ce-c4fa3855bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(df, test_size=0.1)\n",
    "train_data.to_parquet('train_data.parquet')\n",
    "val_data.to_parquet('val_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "e922c144-cd0c-49d6-9444-63916e50e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dim=50):\n",
    "    weights = np.vstack(pickle.load(open('bert-base-cased.pkl', 'rb')))\n",
    "    embedding_col = Input(name='embedding_col', shape=(1, ))\n",
    "    question_embedding = Embedding(weights.shape[0], weights.shape[1], weights=[weights], name='question_embedding')(embedding_col)\n",
    "    question_embedding.trainable = False\n",
    "    x2 = Dense(50, name=f'question_embedding2')(question_embedding)\n",
    "    \n",
    "    x = [x2]\n",
    "    x = tf.concat(x, axis=1, name='concat1')\n",
    "    \n",
    "    for i in range(1):\n",
    "        x = tfrs.layers.dcn.Cross(projection_dim=dim*3, kernel_initializer=\"glorot_uniform\", name=f'cross_layer_{i}')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "    \n",
    "    for i in range(1):\n",
    "        x = Dense(dim*3, activation=\"relu\", name=f'dense_layer_{i}')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "    \n",
    "    out = Dense(1, activation='sigmoid', name=\"out\")(x)\n",
    "    inputs = {'embedding_col': embedding_col}\n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "188489d7-5612-40bf-b735-c6ad0b3137cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train, val):\n",
    "    l = list(train['embedding_col'])\n",
    "    lowest_val = 4.3502\n",
    "    l = [i + lowest_val for i in l]\n",
    "    l2 = list(np.zeros((900, 768)))\n",
    "    tensor = tf.convert_to_tensor(l)\n",
    "    tensor2 = tf.convert_to_tensor(l2)\n",
    "    model = get_model()\n",
    "\n",
    "    train_label = np.array(train['binarized'])\n",
    "    val_features = {\n",
    "        'embedding_col': np.array(val['embedding_col'])\n",
    "    }\n",
    "    val_label = np.array(val['binarized'])\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=2e-05,\n",
    "        decay_steps=80000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        tensor, \n",
    "        np.random.choice([0, 1], size=(900, 768), p=[0.7, 0.3]),\n",
    "        batch_size=8,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        use_multiprocessing=True,\n",
    "        workers=20\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "95dc42d3-398f-4d55-b9c6-f2e6dced2cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 2s 13ms/step - loss: 0.6651 - auc_100: 0.4996 - binary_accuracy: 0.6533\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 1s 13ms/step - loss: 0.6263 - auc_100: 0.5004 - binary_accuracy: 0.6927\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 1s 13ms/step - loss: 0.6205 - auc_100: 0.5003 - binary_accuracy: 0.6972\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 2s 13ms/step - loss: 0.6179 - auc_100: 0.5010 - binary_accuracy: 0.6981\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 2s 13ms/step - loss: 0.6166 - auc_100: 0.5009 - binary_accuracy: 0.6984\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 1s 13ms/step - loss: 0.6158 - auc_100: 0.5008 - binary_accuracy: 0.6985\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 1s 13ms/step - loss: 0.6153 - auc_100: 0.4995 - binary_accuracy: 0.6985\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 1s 13ms/step - loss: 0.6149 - auc_100: 0.5002 - binary_accuracy: 0.6986\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 1s 13ms/step - loss: 0.6145 - auc_100: 0.5011 - binary_accuracy: 0.6986\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 2s 13ms/step - loss: 0.6144 - auc_100: 0.5002 - binary_accuracy: 0.6986\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_parquet('train_data.parquet')\n",
    "val_data = pd.read_parquet('val_data.parquet')\n",
    "trained_model = train(train_data, val_data)\n",
    "trained_model.save_weights('missing_imputation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "b2a2b40b-5012-474a-abd0-7fd9203ce188",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_val = 4.3502\n",
    "input = [i + lowest_val for i in embeddings_fixed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "28b7de3a-b1ee-4894-992b-432d285b6ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 548us/step\n",
      "[[[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3063426 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3063426 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3063426 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3020401 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3063426 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3063426 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3063426 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3063426 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3020401 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.32185972]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31629544]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.31273463]]\n",
      "\n",
      " [[0.3085397 ]]\n",
      "\n",
      " [[0.3097434 ]]]\n"
     ]
    }
   ],
   "source": [
    "prediction_model = get_model()\n",
    "prediction_model.load_weights('missing_imputation.h5')\n",
    "obs =  np.random.choice([0, 1], size=(900, 768), p=[0.7, 0.3])\n",
    "pred = prediction_model.predict(input[0])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134bca9-4800-4406-a5f8-611c7ef4da18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
